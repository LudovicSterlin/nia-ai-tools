{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDPs in pymdptoolbox - praticals (github@pucrs-automated-planning/mdp)\n",
    "\n",
    "In this practical exercise, we will look at how MDP planning is implemented in a mathematical toolkit, and track the calculation of the rewards for each state via Value Iteration. The following code sets up an MDP environment (the basic case shown in class, shown in the Figure below) and computes the policy for the given MDP using the Value Iteration algorithm.\n",
    "\n",
    "<img align=\"center\" src=\"./figures/maze-example.png\" width=300/>\n",
    "Then we provide a set of questions for you to implement and answer. \n",
    "This assignment is not graded.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['E' 'E' 'E' 'T']\n ['N' 'O' 'N' 'T']\n ['N' 'W' 'N' 'W']]\n"
     ]
    }
   ],
   "source": [
    "# The line below is to be used if you have pymdptoolbox installed with setuptools\n",
    "# import mdptoolbox.example\n",
    "# Whereas the line below obviate the need to install that\n",
    "import sys\n",
    "sys.path.insert(0, 'pymdptoolbox/src')\n",
    "import mdptoolbox.example\n",
    "\n",
    "import numpy as _np\n",
    "from gen_scenario import *\n",
    "\n",
    "\"\"\"\n",
    "(Y,X)\n",
    "| 00 01 02 ... 0X-1       'N' = North\n",
    "| 10  .         .         'S' = South\n",
    "| 20    .       .         'W' = West\n",
    "| .       .     .         'E' = East\n",
    "| .         .   .         'T' = Terminal\n",
    "| .           . .         'O' = Obstacle\n",
    "| Y-1,0 . . .   Y-1X-1\n",
    "\"\"\" \n",
    "\n",
    "shape = [3,4]\n",
    "rewards = [[0,3,1],[1,3,-1]]\n",
    "obstacles = [[1,1]]\n",
    "terminals = [[0,3],[1,3]] # note this states are considered as terminal states\n",
    "P, R = mdp_grid(shape=shape, terminals=terminals, r=-0.1, rewards=rewards, obstacles=obstacles)\n",
    "vi = mdptoolbox.mdp.ValueIteration(P, R, discount=0.99, epsilon=0.001, max_iter=1000, skip_check=True)\n",
    "vi.run()\n",
    "#You can check the quadrant values using print vi.V\n",
    "print_policy(vi.policy, shape, obstacles=obstacles, terminals=terminals)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you go for the questionnaire, take your time to open the sourcecode of the MDP toolkit we use, specifically, look into these files:\n",
    "1. [gen_scenario.py](gen_scenario.py) - contains the conversion code to make the simple coordinate commands above (e.g. ```shape = [3,4]```) into the matrices actually used by the MDP solver\n",
    "2. [mdp.py](pymdptoolbox/src/mdptoolbox/mdp.py) - contains most of the logic for an MDP, including the *Bellman Equation* as follows:\n",
    "\n",
    "$$V(s) = \\max_{a} \\left[ R(s,a)+ \\gamma \\sum_{s'}P(s'|s,a)*V(s') \\right]$$\n",
    "\n",
    "See if you can identify where how this equation is implemented in the ```MDP._bellmanOperator``` with the [mdp.py](pymdptoolbox/src/mdptoolbox/mdp.py) file. Note how this implementation uses matrix multiplication to achieve the summation step described in the equation. Once you believe you understand that, go ahead and respond the questionnaire. \n",
    "\n",
    "### Questionnaire\n",
    "1. Study the code of the cell above and answer the following questions.\n",
    "\t1. What is the policy generated if we change the discount factor of the grid domain to 0.1?\n",
    "\t2. Use the following line ```vi.verbose = True\t``` before  ```vi.run()```:   \n",
    "\tWhat is the variation for each of the first three iterations with the discount factor of 0.9 and how many iterations does the algorithm take to converge?\n",
    "\t3. How does changes to the discount factor affect the variation of the state values over time?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " Iteration   Variation\n         1    2.000000\n         2    0.096800\n         3    0.009152\nIterating stopped due to maximum number of iterations condition.\n[['E' 'E' 'E' 'T']\n ['N' 'O' 'W' 'T']\n ['N' 'N' 'N' 'S']]\n"
     ]
    }
   ],
   "source": [
    "#1.A\n",
    "vi = mdptoolbox.mdp.ValueIteration(P, R, discount=0.1, epsilon=0.001, max_iter=1000, skip_check=True)\n",
    "vi.verbose = True\n",
    "vi.run()\n",
    "#You can check the quadrant values using print vi.V\n",
    "print_policy(vi.policy, shape, obstacles=obstacles, terminals=terminals)"
   ]
  },
  {
   "source": [
    "Converge fast but is dumb"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " Iteration   Variation\n         1    2.000000\n         2    0.871200\n         3    0.741312\n         4    0.574802\n         5    0.407370\n         6    0.340194\n         7    0.203374\n         8    0.114344\n         9    0.055497\n        10    0.026025\n        11    0.011618\n        12    0.005090\n        13    0.002180\n        14    0.000924\n        15    0.000387\n        16    0.000161\n        17    0.000066\nIterating stopped, epsilon-optimal policy found.\n[['E' 'E' 'E' 'T']\n ['N' 'O' 'N' 'T']\n ['N' 'E' 'N' 'W']]\n"
     ]
    }
   ],
   "source": [
    "#1.B\n",
    "vi = mdptoolbox.mdp.ValueIteration(P, R, discount=0.9, epsilon=0.001, max_iter=1000, skip_check=True)\n",
    "vi.verbose = True\n",
    "vi.run()\n",
    "#You can check the quadrant values using print vi.V\n",
    "print_policy(vi.policy, shape, obstacles=obstacles, terminals=terminals)"
   ]
  },
  {
   "source": [
    "1    2.000000\n",
    "2    0.871200\n",
    "3    0.741312\n",
    "\n",
    "Need 17 iterations"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#1.C \n",
    "\n",
    "More the dicount is high more the variation is low."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. \n",
    "The scenario below has an interesting structure whereby the positive rewarding terminal state is partially surrounded by negatively-rewarding states. Program this scenario in pymdptoolbox and compute the optimal policy with a discount factor of 0.99.\n",
    "\n",
    "<img align=\"center\" src=\"./figures/maze-example-2.png\" width=300/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['E' 'E' 'E' 'E' 'S']\n ['N' 'O' 'T' 'O' 'S']\n ['N' 'T' 'T' 'W' 'W']\n ['S' 'O' 'T' 'O' 'N']\n ['E' 'E' 'E' 'E' 'N']]\n"
     ]
    }
   ],
   "source": [
    "#2\n",
    "shape = [5,5]\n",
    "rewards = [[1,2,-1],[2,1,-1],[2,2,1],[3,2,-1]]\n",
    "obstacles = [[1,1],[1,3],[3,1],[3,3]]\n",
    "terminals = [[1,2],[2,1],[2,2],[3,2]] # note this states are considered as terminal states\n",
    "P, R = mdp_grid(shape=shape, terminals=terminals, r=-0.1, rewards=rewards, obstacles=obstacles)\n",
    "vi = mdptoolbox.mdp.ValueIteration(P, R, discount=0.99, epsilon=0.001, max_iter=1000, skip_check=True)\n",
    "vi.run()\n",
    "#You can check the quadrant values using print vi.V\n",
    "print_policy(vi.policy, shape, obstacles=obstacles, terminals=terminals)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Define two new 5 by 5 scenarios with multiple obstacles and an interesting geometry following the guidelines below. Calculate the policy with discount factor 0.99, and then try to explain intuitively the reason for the resulting policies, given the initial parameters. These two scenarios must have the following characteristics:\n",
    "\t1. A scenario with one (or more) terminal states with positive rewards and at least one other state with the same amount of, but negative reward and no terminal states with negative rewards.\n",
    "\t2. A scenario with one terminal state with a negative reward and at least one non-terminal state with a positive reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['E' 'E' 'E' 'E' 'S']\n ['N' 'O' 'S' 'O' 'S']\n ['E' 'E' 'T' 'W' 'W']\n ['S' 'O' 'N' 'O' 'N']\n ['E' 'E' 'E' 'E' 'N']]\n"
     ]
    }
   ],
   "source": [
    "#3.A\n",
    "shape = [5,5]\n",
    "rewards = [[1,2,-1],[2,1,-1],[2,2,1],[3,2,-1]]\n",
    "obstacles = [[1,1],[1,3],[3,1],[3,3]]\n",
    "terminals = [[2,2]] # note this states are considered as terminal states\n",
    "P, R = mdp_grid(shape=shape, terminals=terminals, r=-0.1, rewards=rewards, obstacles=obstacles)\n",
    "vi = mdptoolbox.mdp.ValueIteration(P, R, discount=0.99, epsilon=0.001, max_iter=1000, skip_check=True)\n",
    "vi.run()\n",
    "#You can check the quadrant values using print vi.V\n",
    "print_policy(vi.policy, shape, obstacles=obstacles, terminals=terminals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['S' 'E' 'S' 'W' 'W']\n ['S' 'O' 'S' 'O' 'S']\n ['E' 'E' 'N' 'W' 'W']\n ['N' 'O' 'T' 'O' 'N']\n ['N' 'W' 'S' 'E' 'N']]\n"
     ]
    }
   ],
   "source": [
    "#3.B\n",
    "shape = [5,5]\n",
    "rewards = [[1,2,1],[2,1,1],[2,2,1],[3,2,-1]]\n",
    "obstacles = [[1,1],[1,3],[3,1],[3,3]]\n",
    "terminals = [[3,2]] # note this states are considered as terminal states\n",
    "P, R = mdp_grid(shape=shape, terminals=terminals, r=-0.1, rewards=rewards, obstacles=obstacles)\n",
    "vi = mdptoolbox.mdp.ValueIteration(P, R, discount=0.99, epsilon=0.001, max_iter=1000, skip_check=True)\n",
    "vi.run()\n",
    "#You can check the quadrant values using print vi.V\n",
    "print_policy(vi.policy, shape, obstacles=obstacles, terminals=terminals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Chose one of previous scenarios, and apply mdptoolbox.mdp.PolicyIteration.\n",
    "\n",
    "    A . How many iterations are needed to reach the optimal policy ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.A"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}